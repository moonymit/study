{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last' 'bid' 'ask' 'low' 'high' 'volume']\n",
      "[[ 0.52493075  0.37053353  0.52463567  0.60770975  0.54615385  0.32157781]\n",
      " [ 0.59141274  0.41752325  0.59333796  0.60770975  0.52820513  0.32015694]]\n",
      "[[ 0.52493075  0.37053353  0.52463567  0.60770975  0.54615385  0.32157781]\n",
      " [ 0.59141274  0.41752325  0.59333796  0.60770975  0.52820513  0.32015694]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "#Min Max Normalization\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 6\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# bid ask low high volume\n",
    "xy = np.genfromtxt('./data/bitcoin_ticker_eth_krw_hour.csv', delimiter=',', dtype=np.str)[:,[4,7,8,9,10,11]]\n",
    "print(xy[0])\n",
    "\n",
    "xy = xy[1:].astype(np.float)\n",
    "xy = MinMaxScaler(xy[1:]) #normalize\n",
    "x = xy[:]\n",
    "y = xy[:]\n",
    "print(x[:2])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52493075  0.37053353  0.52463567  0.60770975  0.54615385  0.32157781]\n",
      " [ 0.59141274  0.41752325  0.59333796  0.60770975  0.52820513  0.32015694]\n",
      " [ 0.55817175  0.39402839  0.57321305  0.60770975  0.52820513  0.32937215]\n",
      " [ 0.60249307  0.39255996  0.59750173  0.60770975  0.52820513  0.33856945]\n",
      " [ 0.50069252  0.35340186  0.50104094  0.60770975  0.52820513  0.34429369]\n",
      " [ 0.51385042  0.35829662  0.50728661  0.60770975  0.52820513  0.35179124]\n",
      " [ 0.47368421  0.33431229  0.48507981  0.60770975  0.51623932  0.34650965]] ->\n",
      "[[ 0.48268698  0.33969652  0.48160999  0.60770975  0.48717949  0.33841812]\n",
      " [ 0.47437673  0.33431229  0.47328244  0.60770975  0.48717949  0.32262733]\n",
      " [ 0.49030471  0.34605972  0.50451076  0.60770975  0.46324786  0.31372875]\n",
      " [ 0.52700831  0.37151248  0.52602359  0.60770975  0.44444444  0.32694262]\n",
      " [ 0.54085873  0.38179148  0.56141568  0.60770975  0.44444444  0.34934979]\n",
      " [ 0.51731302  0.36514929  0.52047189  0.60770975  0.44444444  0.35211733]\n",
      " [ 0.50138504  0.35389134  0.51075642  0.60770975  0.44444444  0.34040645]]\n",
      "[[ 0.59141274  0.41752325  0.59333796  0.60770975  0.52820513  0.32015694]\n",
      " [ 0.55817175  0.39402839  0.57321305  0.60770975  0.52820513  0.32937215]\n",
      " [ 0.60249307  0.39255996  0.59750173  0.60770975  0.52820513  0.33856945]\n",
      " [ 0.50069252  0.35340186  0.50104094  0.60770975  0.52820513  0.34429369]\n",
      " [ 0.51385042  0.35829662  0.50728661  0.60770975  0.52820513  0.35179124]\n",
      " [ 0.47368421  0.33431229  0.48507981  0.60770975  0.51623932  0.34650965]\n",
      " [ 0.48268698  0.33969652  0.48160999  0.60770975  0.48717949  0.33841812]] ->\n",
      "[[ 0.47437673  0.33431229  0.47328244  0.60770975  0.48717949  0.32262733]\n",
      " [ 0.49030471  0.34605972  0.50451076  0.60770975  0.46324786  0.31372875]\n",
      " [ 0.52700831  0.37151248  0.52602359  0.60770975  0.44444444  0.32694262]\n",
      " [ 0.54085873  0.38179148  0.56141568  0.60770975  0.44444444  0.34934979]\n",
      " [ 0.51731302  0.36514929  0.52047189  0.60770975  0.44444444  0.35211733]\n",
      " [ 0.50138504  0.35389134  0.51075642  0.60770975  0.44444444  0.34040645]\n",
      " [ 0.48614958  0.34165443  0.48507981  0.60770975  0.44444444  0.32033049]]\n"
     ]
    }
   ],
   "source": [
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length*2):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length : i + seq_length*2]  # Next last price\n",
    "    if i < 2 : \n",
    "        print(_x, \"->\")\n",
    "        print(_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    \n",
    "# train/test split\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input place holders\n",
    "enc_input= tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "dec_input = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "# encoder cell\n",
    "with tf.variable_scope('encode'):\n",
    "#     enc_cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, activation=tf.tanh, reuse=True)\n",
    "    enc_cell = tf.contrib.rnn.BasicRNNCell(hidden_dim, reuse=True)\n",
    "    enc_cell = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "\n",
    "# decoder cell\n",
    "with tf.variable_scope('decode'):\n",
    "#     dec_cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh, reuse=True)\n",
    "    dec_cell = tf.contrib.rnn.BasicRNNCell(hidden_dim, reuse=True)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    # put encoder enc_states to decoder as initial_state\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, \n",
    "                                            initial_state=enc_states, \n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "    \n",
    "model = tf.layers.dense(outputs, output_dim, activation=None)\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
